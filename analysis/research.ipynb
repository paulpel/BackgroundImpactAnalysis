{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import ast\n",
    "from analyze_masks import analyze_masks_and_list_exceptions\n",
    "\n",
    "def categorize_data(data, column):\n",
    "    \"\"\" Categorize the data based on quantiles for a specific column \"\"\"\n",
    "    # Calculate the 33rd and 66th percentiles\n",
    "    q33 = data[column].quantile(0.33)\n",
    "    q66 = data[column].quantile(0.66)\n",
    "    \n",
    "    # Define the category labels\n",
    "    category_labels = [\"small\", \"medium\", \"large\"]\n",
    "    \n",
    "    # Create a new column for categories based on the quantiles\n",
    "    conditions = [\n",
    "        (data[column] <= q33),\n",
    "        (data[column] > q33) & (data[column] <= q66),\n",
    "        (data[column] > q66)\n",
    "    ]\n",
    "    \n",
    "    data[f'{column}_category'] = np.select(conditions, category_labels)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(file_path, base_dir, masks_dir):\n",
    "    data = pd.read_csv(file_path)\n",
    "    confidence_columns = [col for col in data.columns if 'confidence' in col]\n",
    "    for column in confidence_columns:\n",
    "        data[column] = data[column].apply(ast.literal_eval)\n",
    "\n",
    "    data['width'] = 0\n",
    "    data['height'] = 0\n",
    "    data['object_percentage'] = 0.0\n",
    "\n",
    "    class_to_grayscale_map = analyze_masks_and_list_exceptions(masks_dir)\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        class_id = row['id'].split('_')[0]\n",
    "        picture_name = row['picture_name']\n",
    "        picture_base_name = os.path.splitext(picture_name)[0]\n",
    "\n",
    "        img_path = os.path.join(base_dir, 'train', class_id, picture_name)\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                width, height = img.size\n",
    "                data.at[index, 'width'] = width\n",
    "                data.at[index, 'height'] = height\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Image not found: {img_path}\")\n",
    "\n",
    "        mask_name = picture_base_name + '.png'\n",
    "        mask_path = os.path.join(masks_dir, class_id, mask_name)\n",
    "        try:\n",
    "            with Image.open(mask_path) as mask:\n",
    "                mask_array = np.array(mask)\n",
    "                if class_id in class_to_grayscale_map:\n",
    "                    relevant_value = class_to_grayscale_map[class_id]\n",
    "                    object_pixels = np.sum(mask_array == relevant_value)\n",
    "                    total_pixels = width * height\n",
    "                    data.at[index, 'object_percentage'] = (object_pixels / total_pixels) * 100\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Mask not found for image: {mask_path}\")\n",
    "\n",
    "    data = categorize_data(data, 'object_percentage')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_synset_mapping(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        class_dict = {}\n",
    "        for row_number, line in enumerate(file, start=1):\n",
    "            class_id, description = line.strip().split(' ', 1)\n",
    "            class_dict[class_id] = {\n",
    "                \"description\": description,\n",
    "                \"value\": row_number - 1\n",
    "            }\n",
    "    return class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: n02412080, Most Common Nonzero Grayscale Value (by presence): 17, Presence Count: 982\n",
      "    Images without the most common grayscale value (17): ['n02412080_13145.png', 'n02412080_1976.png', 'n02412080_6399.png', 'n02412080_16811.png', 'n02412080_2188.png', 'n02412080_19324.png', 'n02412080_18733.png', 'n02412080_2270.png', 'n02412080_16830.png', 'n02412080_16254.png', 'n02412080_26458.png', 'n02412080_3944.png', 'n02412080_1040.png', 'n02412080_10804.png', 'n02412080_13818.png', 'n02412080_11852.png', 'n02412080_791.png', 'n02412080_17063.png']\n",
      "Class: n02107574, Most Common Nonzero Grayscale Value (by presence): 12, Presence Count: 997\n",
      "    Images without the most common grayscale value (12): ['n02107574_3660.png', 'n02107574_142.png', 'n02107574_690.png']\n",
      "Class: n01833805, Most Common Nonzero Grayscale Value (by presence): 3, Presence Count: 987\n",
      "    Images without the most common grayscale value (3): ['n01833805_4117.png', 'n01833805_166.png', 'n01833805_8510.png', 'n01833805_8856.png', 'n01833805_7727.png', 'n01833805_8820.png', 'n01833805_8424.png', 'n01833805_5809.png', 'n01833805_6432.png', 'n01833805_644.png', 'n01833805_8396.png', 'n01833805_8841.png', 'n01833805_179.png']\n",
      "Class: n02124075, Most Common Nonzero Grayscale Value (by presence): 8, Presence Count: 995\n",
      "    Images without the most common grayscale value (8): ['n02124075_9907.png', 'n02124075_9372.png', 'n02124075_990.png', 'n02124075_4220.png', 'n02124075_10681.png']\n",
      "Class: n02415577, Most Common Nonzero Grayscale Value (by presence): 17, Presence Count: 961\n",
      "    Images without the most common grayscale value (17): ['n02415577_19492.png', 'n02415577_940.png', 'n02415577_9146.png', 'n02415577_330.png', 'n02415577_196.png', 'n02415577_3122.png', 'n02415577_9641.png', 'n02415577_7961.png', 'n02415577_50425.png', 'n02415577_431.png', 'n02415577_1089.png', 'n02415577_2958.png', 'n02415577_3991.png', 'n02415577_1224.png', 'n02415577_5365.png', 'n02415577_173.png', 'n02415577_7388.png', 'n02415577_28963.png', 'n02415577_2606.png', 'n02415577_458.png', 'n02415577_1236.png', 'n02415577_563.png', 'n02415577_8933.png', 'n02415577_18708.png', 'n02415577_11446.png', 'n02415577_6598.png', 'n02415577_3593.png', 'n02415577_90.png', 'n02415577_9582.png', 'n02415577_4097.png', 'n02415577_2436.png', 'n02415577_741.png', 'n02415577_2813.png', 'n02415577_6163.png', 'n02415577_1198.png', 'n02415577_1097.png', 'n02415577_7470.png', 'n02415577_604.png', 'n02415577_19471.png']\n",
      "Class: n02105641, Most Common Nonzero Grayscale Value (by presence): 12, Presence Count: 977\n",
      "    Images without the most common grayscale value (12): ['n02105641_6294.png', 'n02105641_13572.png', 'n02105641_15011.png', 'n02105641_5824.png', 'n02105641_12310.png', 'n02105641_7062.png', 'n02105641_5873.png', 'n02105641_6994.png', 'n02105641_12240.png', 'n02105641_14742.png', 'n02105641_5227.png', 'n02105641_10031.png', 'n02105641_1117.png', 'n02105641_5945.png', 'n02105641_3526.png', 'n02105641_3189.png', 'n02105641_12876.png', 'n02105641_8698.png', 'n02105641_14139.png', 'n02105641_10967.png', 'n02105641_988.png', 'n02105641_536.png', 'n02105641_1647.png']\n",
      "Class: n02123394, Most Common Nonzero Grayscale Value (by presence): 8, Presence Count: 988\n",
      "    Images without the most common grayscale value (8): ['n02123394_167.png', 'n02123394_7089.png', 'n02123394_8992.png', 'n02123394_6363.png', 'n02123394_757.png', 'n02123394_8165.png', 'n02123394_7051.png', 'n02123394_4670.png', 'n02123394_2804.png', 'n02123394_578.png', 'n02123394_2357.png', 'n02123394_28.png']\n",
      "Class: n01534433, Most Common Nonzero Grayscale Value (by presence): 3, Presence Count: 1027\n",
      "    Images without the most common grayscale value (3): ['n01534433_14192.png', 'n01534433_14325.png', 'n01534433_14360.png']\n",
      "Class: n02106662, Most Common Nonzero Grayscale Value (by presence): 12, Presence Count: 997\n",
      "    Images without the most common grayscale value (12): ['n02106662_8045.png', 'n02106662_11501.png', 'n02106662_25465.png']\n",
      "Class: n01558993, Most Common Nonzero Grayscale Value (by presence): 3, Presence Count: 999\n",
      "    Images without the most common grayscale value (3): ['n01558993_14295.png']\n",
      "Class: n02412080, Most Common Nonzero Grayscale Value (by presence): 17, Presence Count: 982\n",
      "    Images without the most common grayscale value (17): ['n02412080_13145.png', 'n02412080_1976.png', 'n02412080_6399.png', 'n02412080_16811.png', 'n02412080_2188.png', 'n02412080_19324.png', 'n02412080_18733.png', 'n02412080_2270.png', 'n02412080_16830.png', 'n02412080_16254.png', 'n02412080_26458.png', 'n02412080_3944.png', 'n02412080_1040.png', 'n02412080_10804.png', 'n02412080_13818.png', 'n02412080_11852.png', 'n02412080_791.png', 'n02412080_17063.png']\n",
      "Class: n02107574, Most Common Nonzero Grayscale Value (by presence): 12, Presence Count: 997\n",
      "    Images without the most common grayscale value (12): ['n02107574_3660.png', 'n02107574_142.png', 'n02107574_690.png']\n",
      "Class: n01833805, Most Common Nonzero Grayscale Value (by presence): 3, Presence Count: 987\n",
      "    Images without the most common grayscale value (3): ['n01833805_4117.png', 'n01833805_166.png', 'n01833805_8510.png', 'n01833805_8856.png', 'n01833805_7727.png', 'n01833805_8820.png', 'n01833805_8424.png', 'n01833805_5809.png', 'n01833805_6432.png', 'n01833805_644.png', 'n01833805_8396.png', 'n01833805_8841.png', 'n01833805_179.png']\n",
      "Class: n02124075, Most Common Nonzero Grayscale Value (by presence): 8, Presence Count: 995\n",
      "    Images without the most common grayscale value (8): ['n02124075_9907.png', 'n02124075_9372.png', 'n02124075_990.png', 'n02124075_4220.png', 'n02124075_10681.png']\n",
      "Class: n02415577, Most Common Nonzero Grayscale Value (by presence): 17, Presence Count: 961\n",
      "    Images without the most common grayscale value (17): ['n02415577_19492.png', 'n02415577_940.png', 'n02415577_9146.png', 'n02415577_330.png', 'n02415577_196.png', 'n02415577_3122.png', 'n02415577_9641.png', 'n02415577_7961.png', 'n02415577_50425.png', 'n02415577_431.png', 'n02415577_1089.png', 'n02415577_2958.png', 'n02415577_3991.png', 'n02415577_1224.png', 'n02415577_5365.png', 'n02415577_173.png', 'n02415577_7388.png', 'n02415577_28963.png', 'n02415577_2606.png', 'n02415577_458.png', 'n02415577_1236.png', 'n02415577_563.png', 'n02415577_8933.png', 'n02415577_18708.png', 'n02415577_11446.png', 'n02415577_6598.png', 'n02415577_3593.png', 'n02415577_90.png', 'n02415577_9582.png', 'n02415577_4097.png', 'n02415577_2436.png', 'n02415577_741.png', 'n02415577_2813.png', 'n02415577_6163.png', 'n02415577_1198.png', 'n02415577_1097.png', 'n02415577_7470.png', 'n02415577_604.png', 'n02415577_19471.png']\n",
      "Class: n02105641, Most Common Nonzero Grayscale Value (by presence): 12, Presence Count: 977\n",
      "    Images without the most common grayscale value (12): ['n02105641_6294.png', 'n02105641_13572.png', 'n02105641_15011.png', 'n02105641_5824.png', 'n02105641_12310.png', 'n02105641_7062.png', 'n02105641_5873.png', 'n02105641_6994.png', 'n02105641_12240.png', 'n02105641_14742.png', 'n02105641_5227.png', 'n02105641_10031.png', 'n02105641_1117.png', 'n02105641_5945.png', 'n02105641_3526.png', 'n02105641_3189.png', 'n02105641_12876.png', 'n02105641_8698.png', 'n02105641_14139.png', 'n02105641_10967.png', 'n02105641_988.png', 'n02105641_536.png', 'n02105641_1647.png']\n",
      "Class: n02123394, Most Common Nonzero Grayscale Value (by presence): 8, Presence Count: 988\n",
      "    Images without the most common grayscale value (8): ['n02123394_167.png', 'n02123394_7089.png', 'n02123394_8992.png', 'n02123394_6363.png', 'n02123394_757.png', 'n02123394_8165.png', 'n02123394_7051.png', 'n02123394_4670.png', 'n02123394_2804.png', 'n02123394_578.png', 'n02123394_2357.png', 'n02123394_28.png']\n",
      "Class: n01534433, Most Common Nonzero Grayscale Value (by presence): 3, Presence Count: 1027\n",
      "    Images without the most common grayscale value (3): ['n01534433_14192.png', 'n01534433_14325.png', 'n01534433_14360.png']\n",
      "Class: n02106662, Most Common Nonzero Grayscale Value (by presence): 12, Presence Count: 997\n",
      "    Images without the most common grayscale value (12): ['n02106662_8045.png', 'n02106662_11501.png', 'n02106662_25465.png']\n",
      "Class: n01558993, Most Common Nonzero Grayscale Value (by presence): 3, Presence Count: 999\n",
      "    Images without the most common grayscale value (3): ['n01558993_14295.png']\n"
     ]
    }
   ],
   "source": [
    "base_dir = '../data'\n",
    "masks_dir = '../data/masks'\n",
    "file_path_resnet = '../image_confidence_scores_resnet.csv'\n",
    "file_path_convnext = '../image_confidence_scores_convnext.csv'\n",
    "synset_path = '../data/LOC_synset_mapping.txt'\n",
    "\n",
    "data_resnet = load_and_process_data(file_path_resnet, base_dir, masks_dir)\n",
    "data_convnext = load_and_process_data(file_path_convnext, base_dir, masks_dir)\n",
    "class_dict = parse_synset_mapping(synset_path)\n",
    "\n",
    "category_distribution_resnet = data_resnet['object_percentage_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_df(df):\n",
    "    # Save the DataFrame to a pickle file\n",
    "    with open('processed_image_data.pkl', 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "\n",
    "def load_df(path):\n",
    "    # Load the DataFrame from the pickle file\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
